{
  "task_name": "mimic3-full",
  "train_file": "../data/mimic3/full/dev_data_full_raw.csv",
  "validation_file": "../data/mimic3/full/dev_data_full_raw.csv",
  "test_file": "../data/mimic3/full/dev_data_full_raw.csv",
  "label_dictionary_file": "../data/mimic3/full/labels_dictionary_full_raw.csv",
  "ignore_keys_for_eval": ["preds", "label_attention_weights", "chunk_attention_weights"],
  "use_cached_datasets": true,
  "data_segmented": false,

  "transformer_name_or_path": "[your RoBERTa-PM-M3 folder]",
  "transformer_tokenizer_name": "[your RoBERTa-PM-M3 folder]",
  "d_model": 768,
  "dropout": 0.1,
  "dropout_att": 0.1,
  "max_seq_length": 512,
  "num_chunks_per_document": 10,
  "transformer_layer_update_strategy": "all",
  "linear_init_mean": 0.0,
  "linear_init_std": 0.03,
  "document_pooling_strategy": "mean",
  "model_type": "roberta_pm",
  "use_ASL": false,

  "output_dir": "../model/",
  "overwrite_output_dir": true,
  "do_train": true,
  "do_eval": true,
  "do_predict": true,
  "learning_rate": 5e-5,
  "weight_decay": 0.1,
  "evaluation_strategy": "steps",
  "logging_strategy": "steps",
  "save_strategy": "steps",
  "per_device_train_batch_size": 8,
  "per_device_eval_batch_size": 8,
  "gradient_accumulation_steps": 1,
  "num_train_epochs": 100,
  "max_steps": 60000,
  "warmup_steps": 5000,
  "logging_steps": 5000,
  "save_steps": 5000,
  "seed": 2022,
  "log_level": "info",
  "dataloader_drop_last": false,
  "disable_tqdm": false,
  "label_names": ["targets"],
  "load_best_model_at_end": true,
  "metric_for_best_model": "micro_f1",
  "greater_is_better": true,
  "remove_unused_columns": false
}